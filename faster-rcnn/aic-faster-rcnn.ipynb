{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\nimport os\nimport sys\nimport torch\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms, datasets, models\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\nimport time\nfrom collections import Counter\n!pip install xmltodict\nimport xmltodict\nfrom xml.etree import ElementTree as et\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom bs4 import BeautifulSoup","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:53:59.233513Z","iopub.execute_input":"2022-06-10T06:53:59.233929Z","iopub.status.idle":"2022-06-10T06:54:08.231314Z","shell.execute_reply.started":"2022-06-10T06:53:59.233896Z","shell.execute_reply":"2022-06-10T06:54:08.230155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"img_names=[] \nxml_names=[] \nfor dirname, _, filenames in os.walk('/kaggle/input/face-mask-detection/'):\n    for filename in filenames:\n        if os.path.join(dirname, filename)[-3:]!=\"xml\":\n            img_names.append(filename)\n        else:\n            xml_names.append(filename)\n            \npath_annotations=\"/kaggle/input/face-mask-detection/annotations/\" \nlisting=[]\nfor i in img_names[:]:\n    with open(path_annotations+i[:-4]+\".xml\") as fd:\n        doc=xmltodict.parse(fd.read())\n    temp=doc[\"annotation\"][\"object\"]\n    if type(temp)==list:\n        for i in range(len(temp)):\n            listing.append(temp[i][\"name\"])\n    else:\n        listing.append(temp[\"name\"])\n\n\nItems = Counter(listing).keys()\nvalues = Counter(listing).values()\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize =(14,6))\nbackground_color = '#faf9f4'\nax1.set_facecolor(background_color)\nax2.set_facecolor(background_color) \nax1.pie(values,wedgeprops=dict(width=0.3, edgecolor='w') ,\n        labels=Items, radius=1, startangle = 120, autopct='%1.2f%%')\n\nax2 = plt.bar(Items, list(values),\n              color ='maroon',width = 0.4)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:54:08.233778Z","iopub.execute_input":"2022-06-10T06:54:08.234231Z","iopub.status.idle":"2022-06-10T06:54:09.596259Z","shell.execute_reply.started":"2022-06-10T06:54:08.234175Z","shell.execute_reply":"2022-06-10T06:54:09.59497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_dir = '../input/face-mask-detection/images/'\nannotations_dir = '../input/face-mask-detection/annotations/'\n\n'''!mkdir train_data\n!mkdir test_data\n!cp -r ../input/yolov5trainedep99/yolov5/data/train/images train_data\n!cp -r ../input/yolov5trainedep99/yolov5/data/train/labels train_data\n!cp -r ../input/yolov5trainedep99/yolov5/data/val/images/* train_data/images\n!cp -r ../input/yolov5trainedep99/yolov5/data/val/labels/* train_data/labels\n!cp -r ../input/yolov5trainedep99/yolov5/data/test/* test_data'''","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:54:09.59907Z","iopub.execute_input":"2022-06-10T06:54:09.599559Z","iopub.status.idle":"2022-06-10T06:54:09.609638Z","shell.execute_reply.started":"2022-06-10T06:54:09.599512Z","shell.execute_reply":"2022-06-10T06:54:09.608131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, images_dir, annotation_dir,width, height, transforms=None):\n        self.transforms = transforms\n        self.images_dir = images_dir\n        self.annotation_dir = annotation_dir\n        self.height = height\n        self.width = width\n        \n        self.imgs = [image for image in sorted(os.listdir(images_dir))]\n        self.annotate = [image for image in sorted(os.listdir(annotation_dir))]\n        \n        self.classes = [_, 'without_mask','with_mask','mask_weared_incorrect']\n\n    def __getitem__(self, idx):\n        img_name = self.imgs[idx]\n        image_path = os.path.join(self.images_dir, img_name)\n        \n        # BGR2RGB, resize and normalized evey images before transform()\n        img = cv2.imread(image_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n        img_res /= 255.0\n        \n        annot_filename = self.annotate[idx]\n        annot_file_path = os.path.join(self.annotation_dir, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        for member in root.findall('object'):\n            labels.append(self.classes.index(member.find('name').text))\n            \n            xmin = int(member.find('bndbox').find('xmin').text)\n            xmax = int(member.find('bndbox').find('xmax').text)\n            ymin = int(member.find('bndbox').find('ymin').text)\n            ymax = int(member.find('bndbox').find('ymax').text)\n            \n            xmin_corr = (xmin/wt)*self.width\n            xmax_corr = (xmax/wt)*self.width\n            ymin_corr = (ymin/ht)*self.height\n            ymax_corr = (ymax/ht)*self.height\n            \n            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n        \n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        \n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n        if self.transforms:\n            sample = self.transforms(image = img_res, \n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            img_res = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n        return img_res, target\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:54:09.612133Z","iopub.execute_input":"2022-06-10T06:54:09.612708Z","iopub.status.idle":"2022-06-10T06:54:09.634927Z","shell.execute_reply.started":"2022-06-10T06:54:09.612661Z","shell.execute_reply":"2022-06-10T06:54:09.63333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_img_bbox(img, target):\n    fig, a = plt.subplots(1,1) \n    fig.set_size_inches(5,5) \n    a.imshow(img, cmap='gray')\n    for box in (target['boxes']):\n        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = patches.Rectangle((x, y),\n                                 width, height,\n                                 linewidth = 2,\n                                 edgecolor = 'white',\n                                 facecolor = 'none')\n\n        a.add_patch(rect)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:54:09.673588Z","iopub.execute_input":"2022-06-10T06:54:09.674296Z","iopub.status.idle":"2022-06-10T06:54:09.683844Z","shell.execute_reply.started":"2022-06-10T06:54:09.674221Z","shell.execute_reply":"2022-06-10T06:54:09.682609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transform(train):\n    if train:\n        return A.Compose([\n                            A.OneOf([\n                                A.MotionBlur(p=0.2),\n                                A.MedianBlur(blur_limit=3, p=0.2),\n                                A.Blur(blur_limit=3, p=0.2),\n                            ], p=0.4),\n                            A.RandomBrightnessContrast(p=0.1),\n                            A.RandomGamma(gamma_limit=(80, 120), eps=None, always_apply=False, p=0.2),\n                            ToTensorV2(p=1.0)\n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n    else:\n        return A.Compose([\n                            ToTensorV2(p=1.0)\n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:54:09.685788Z","iopub.execute_input":"2022-06-10T06:54:09.686671Z","iopub.status.idle":"2022-06-10T06:54:09.697988Z","shell.execute_reply.started":"2022-06-10T06:54:09.686553Z","shell.execute_reply":"2022-06-10T06:54:09.696679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:54:09.702775Z","iopub.execute_input":"2022-06-10T06:54:09.703302Z","iopub.status.idle":"2022-06-10T06:54:09.712716Z","shell.execute_reply.started":"2022-06-10T06:54:09.703255Z","shell.execute_reply":"2022-06-10T06:54:09.711578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndataset = MaskDataset(images_dir, annotations_dir, 480, 480, transforms= get_transform(train=True))\ndataset_test = MaskDataset(images_dir, annotations_dir, 480, 480, transforms= get_transform(train=False))\n\ntrain_dataset, _ = train_test_split(dataset, test_size=0.1, random_state=22)\n_, test_dataset = train_test_split(dataset_test, test_size=0.1, random_state=22)\ntest_dataset, val_dataset = train_test_split(test_dataset, test_size=0.07, random_state=22)\n\ndata_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=8, shuffle=True, num_workers=4,\n    collate_fn=collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    val_dataset, batch_size=4, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:54:09.715738Z","iopub.execute_input":"2022-06-10T06:54:09.716389Z","iopub.status.idle":"2022-06-10T06:54:43.702068Z","shell.execute_reply.started":"2022-06-10T06:54:09.71634Z","shell.execute_reply":"2022-06-10T06:54:43.700757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def get_model_instance_segmentation(num_classes):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features \n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:54:43.703936Z","iopub.execute_input":"2022-06-10T06:54:43.704415Z","iopub.status.idle":"2022-06-10T06:54:43.71129Z","shell.execute_reply.started":"2022-06-10T06:54:43.704369Z","shell.execute_reply":"2022-06-10T06:54:43.709863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"def IOU(box1, box2):\n    xmin_inter = max(box1[0], box2[0])\n    ymin_inter = max(box1[1], box2[1])\n    xmax_inter = min(box1[2], box2[2])\n    ymax_inter = min(box1[3], box2[3])\n\n    inter_area = max(0, xmax_inter - xmin_inter + 1) * max(0, ymax_inter - ymin_inter + 1) # FIXME why plus one?\n \n    area1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n    area2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n \n    iou = inter_area / float(area1 + area2 - inter_area)\n    assert iou >= 0\n    return iou","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:54:43.713165Z","iopub.execute_input":"2022-06-10T06:54:43.713635Z","iopub.status.idle":"2022-06-10T06:54:43.725885Z","shell.execute_reply.started":"2022-06-10T06:54:43.71359Z","shell.execute_reply":"2022-06-10T06:54:43.724789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_AP(ground_truth, predictions, iou_thresh, n_classes=4):\n    # Initialize lists\n    APs = []\n    class_gt = []\n    class_predictions = []\n    \n    for c in range(n_classes):\n        # Find gt and predictions of the class\n        for gt in ground_truth:\n            if gt[4] == c: # 4->label\n                class_gt.append(gt)\n        for predict in predictions:\n            if predict[4] == c:\n                class_predictions.append(predict)\n                \n        gt_amount_bb = Counter([gt[1] for gt in class_gt])\n        for key, val in gt_amount_bb.items():\n            gt_amount_bb[key] = np.zeros(val)\n\n        class_predictions = sorted(class_predictions, key=lambda x: x[5], reverse=True)\n\n        TP = np.zeros(len(class_predictions))\n        FP = np.zeros(len(class_predictions))\n        truth = len(class_gt)\n        epsilon = 1e-6\n\n        for predict_idx, prediction in enumerate(class_predictions):\n            image_gt = [obj for obj in class_gt if obj[1] == prediction[1]]\n\n            best_iou = -1\n            best_gt_iou_idx = -1\n\n            for gt_idx, gt in enumerate(image_gt):\n                iou = IOU(prediction[3], gt[3])\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_iou_idx = gt_idx\n\n            if best_iou > iou_thresh and best_gt_iou_idx > -1:\n                # Check if gt box was already covered\n                if  gt_amount_bb[prediction[1]][best_gt_iou_idx] == 0:\n                    gt_amount_bb[prediction[1]][best_gt_iou_idx] = 1  # set as covered\n                    TP[predict_idx] = 1  # Count as true positive\n                else:\n                    FP[predict_idx] = 1\n            else:\n                FP[predict_idx] = 1\n\n        # Calculate recall and precision\n        TP_cumsum = np.cumsum(TP)\n        FP_cumsum = np.cumsum(FP)\n        recall = np.append([0], TP_cumsum / (truth + epsilon))\n        precision = np.append([1], np.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon)))\n        AP = np.trapz(precision, recall)\n        APs.append(AP)\n        print(f\"class = {c}, precision = {precision.mean()}, recall = {recall.mean()}, AP = {AP.mean()}\")\n\n    return sum(APs)/3  # average of class precisions","metadata":{"execution":{"iopub.status.busy":"2022-06-10T07:18:54.466761Z","iopub.execute_input":"2022-06-10T07:18:54.467407Z","iopub.status.idle":"2022-06-10T07:18:54.505885Z","shell.execute_reply.started":"2022-06-10T07:18:54.467356Z","shell.execute_reply":"2022-06-10T07:18:54.503082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_mAP(ground_truth, predictions, n_classes):\n    iou_thresh = 0.5\n    mAP = compute_AP(ground_truth, predictions, iou_thresh, n_classes)\n    return mAP","metadata":{"execution":{"iopub.status.busy":"2022-06-10T07:16:04.059493Z","iopub.execute_input":"2022-06-10T07:16:04.059952Z","iopub.status.idle":"2022-06-10T07:16:04.065384Z","shell.execute_reply.started":"2022-06-10T07:16:04.059905Z","shell.execute_reply":"2022-06-10T07:16:04.064059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, data_loader, device, sequences=1):\n    # Set evaluation mode flag\n    model.eval()\n    ground_truth = []\n    predictions = []\n\n    for image, targets in data_loader:\n        image = [img.to(device) for img in image]\n        outputs = model(image)\n        for idx in range(len(outputs)):\n            outputs[idx] = apply_nms(outputs[idx], iou_thresh=0.3)\n\n        for s in range(sequences):\n            obj_gt = 0\n            obj_target = 0\n            for out, target in zip(outputs, targets):# (output, target總batch數量一樣)\n\n                for i in range(len(target['boxes'])): \n                    ground_truth.append([s, target['image_id'].detach().cpu().numpy()[0], obj_target,\n                                         target['boxes'].detach().cpu().numpy()[i],\n                                         target['labels'].detach().cpu().numpy()[i], 1])\n                    obj_target += 1\n\n                for j in range(len(out['boxes'])):# 每個output batch大小不同，因為前面的nms confidence刪掉一些重疊的\n                    predictions.append([s, target['image_id'].detach().cpu().numpy()[0], obj_gt,\n                                        out['boxes'].detach().cpu().numpy()[j],\n                                        out['labels'].detach().cpu().numpy()[j],\n                                        out['scores'].detach().cpu().numpy()[j]])\n                    obj_gt += 1\n\n    mAP = compute_mAP(ground_truth, predictions, n_classes=4)\n\n    return mAP","metadata":{"execution":{"iopub.status.busy":"2022-06-10T07:16:16.891959Z","iopub.execute_input":"2022-06-10T07:16:16.892382Z","iopub.status.idle":"2022-06-10T07:16:16.905819Z","shell.execute_reply.started":"2022-06-10T07:16:16.892349Z","shell.execute_reply":"2022-06-10T07:16:16.903843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh=0.3):\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\ndef torch_to_pil(img):\n    return transforms.ToPILImage()(img).convert('RGB')","metadata":{"execution":{"iopub.status.busy":"2022-06-10T07:19:43.854874Z","iopub.execute_input":"2022-06-10T07:19:43.855283Z","iopub.status.idle":"2022-06-10T07:19:43.862504Z","shell.execute_reply.started":"2022-06-10T07:19:43.855251Z","shell.execute_reply":"2022-06-10T07:19:43.861223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nnum_classes = 4\nmodel = get_model_instance_segmentation(num_classes) \nepochs = 3\nb = 0.1\nmodel.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,momentum=0.9, weight_decay=0.0005)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3,gamma=0.1)  \nlen_dataloader = len(data_loader)\nlen_dataloader_test = len(data_loader_test)\nepoch_loss_min = 1000\nmAP_max = 0\nE = []\nL = []\nLT = []\nM = []\nMT = []\nloss_classifier_mean = []\nloss_classifier_mean_test = []\nloss_box_reg_mean = []\nloss_box_reg_mean_test = []\nloss_objectness_mean = []\nloss_objectness_mean_test = []\nloss_rpn_box_reg_mean = []\nloss_rpn_box_reg_mean_test = []\nfor epoch in range(epochs):\n    print('training:', epoch + 1, 'of', epochs)\n    model.train()\n    epoch_loss = 0\n    epoch_loss_test = 0\n    loss_classifier = []\n    loss_box_reg = []\n    loss_objectness = []\n    loss_rpn_box_reg = []\n    for imgs, annotations in data_loader:\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        loss_dict = model(imgs, annotations)\n\n        loss_classifier.append(loss_dict['loss_classifier'].detach().cpu().numpy())\n        loss_box_reg.append(loss_dict['loss_box_reg'].detach().cpu().numpy())\n        loss_objectness.append(loss_dict['loss_objectness'].detach().cpu().numpy())\n        loss_rpn_box_reg.append(loss_dict['loss_rpn_box_reg'].detach().cpu().numpy())\n\n        losses = sum(loss for loss in loss_dict.values())\n        flood = abs(losses-b)\n\n        optimizer.zero_grad() \n        flood.backward()\n        optimizer.step()\n        epoch_loss += losses.item()\n    epoch_loss /= len_dataloader\n    lc = np.mean(loss_classifier)\n    loss_classifier_mean.append(lc)\n    lbr = np.mean(loss_box_reg)\n    loss_box_reg_mean.append(lbr)\n    lo = np.mean(loss_objectness)\n    loss_objectness_mean.append(lo)\n    lrbr = np.mean(loss_rpn_box_reg)\n    loss_rpn_box_reg_mean.append(lrbr)\n\n    with torch.no_grad():\n        loss_classifier = []\n        loss_box_reg = []\n        loss_objectness = []\n        loss_rpn_box_reg = []\n        for imgs, annotations in data_loader_test:\n            imgs = list(img.to(device) for img in imgs)\n            annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n            loss_dict = model(imgs, annotations)\n\n            loss_classifier.append(loss_dict['loss_classifier'].detach().cpu().numpy())\n            loss_box_reg.append(loss_dict['loss_box_reg'].detach().cpu().numpy())\n            loss_objectness.append(loss_dict['loss_objectness'].detach().cpu().numpy())\n            loss_rpn_box_reg.append(loss_dict['loss_rpn_box_reg'].detach().cpu().numpy())\n\n            losses = sum(loss for loss in loss_dict.values())\n            epoch_loss_test += losses.item()\n        epoch_loss_test /= len_dataloader_test\n        lct = np.mean(loss_classifier)\n        loss_classifier_mean_test.append(lct)\n        lbrt = np.mean(loss_box_reg)\n        loss_box_reg_mean_test.append(lbrt)\n        lot = np.mean(loss_objectness)\n        loss_objectness_mean_test.append(lot)\n        lrbrt = np.mean(loss_rpn_box_reg)\n        loss_rpn_box_reg_mean_test.append(lrbrt)\n\n    E.append(epoch + 1)\n    L.append(epoch_loss)\n    LT.append(epoch_loss_test)\n    model.eval()\n    mAP = evaluate(model, data_loader, device=device)\n    mAP_test = evaluate(model, data_loader_test, device=device)\n    M.append(mAP)\n    MT.append(mAP_test)\n    print(f'Epoch={epoch + 1}, train_loss={epoch_loss}, test_loss={epoch_loss_test},mAP_train ={mAP}, mAP_test ={mAP_test}')\n    if epoch_loss < epoch_loss_min:\n        epoch_loss_min = epoch_loss\n        torch.save(model.state_dict(), 'model_loss_best_10_epochs.pt')\n    if mAP_test > mAP_max:\n        mAP_max = mAP_test\n        torch.save(model.state_dict(), 'model_mAP_best_10_epochs.pt')","metadata":{"execution":{"iopub.status.busy":"2022-06-10T07:20:26.996355Z","iopub.execute_input":"2022-06-10T07:20:26.996715Z","iopub.status.idle":"2022-06-10T07:30:07.896229Z","shell.execute_reply.started":"2022-06-10T07:20:26.99667Z","shell.execute_reply":"2022-06-10T07:30:07.894589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('loss function')\nplt.xlabel('epoch')\nplt.ylabel('epoch-loss')\nplt.plot(E, L) #, color='red')\nplt.plot(E, LT) #, color='blue')\nplt.legend(labels=[\"train\",\"test\"],loc=\"upper right\")\nplt.show()\n\nplt.title('mean Average Precision')\nplt.xlabel('epoch')\nplt.ylabel('mAP')\nplt.plot(E, M) # , color='red')\nplt.plot(E, MT) # , color='blue')\nplt.legend(labels=[\"train\",\"test\"],loc=\"upper right\")\nplt.show()\n\nplt.title('classification loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.plot(E, loss_classifier_mean) #, color='red')\nplt.plot(E, loss_classifier_mean_test) #, color='blue')\nplt.legend(labels=[\"train\",\"test\"],loc=\"upper right\")\nplt.show()\n\nplt.title('regression loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.plot(E, loss_box_reg_mean)\nplt.plot(E, loss_box_reg_mean_test) \nplt.legend(labels=[\"train\",\"test\"],loc=\"upper right\",fontsize=6)\nplt.show()\n\nplt.title('loss_objectness')\nplt.xlabel('epoch')\nplt.ylabel('oss_objectness')\nplt.plot(E, loss_objectness_mean) #, color='red')\nplt.plot(E, loss_objectness_mean_test) #, color='blue')\nplt.legend(labels=[\"loss_box_reg_train\",\"loss_box_reg_test\"],loc=\"upper right\",fontsize=6)\nplt.show()\n\n\nplt.title('loss_rpn_box_reg')\nplt.xlabel('epoch')\nplt.ylabel('loss_rpn_box_reg')\nplt.plot(E, loss_rpn_box_reg_mean) #, color='red')\nplt.plot(E, loss_rpn_box_reg_mean_test) #, color='blue')\nplt.legend(labels=[\"loss_rpn_box_reg_train\",\"loss_rpn_box_reg_test\"],loc=\"upper right\",fontsize=6)\nplt.show()\n\nplt.title('losses of train')\nplt.xlabel('epoch')\nplt.ylabel('losses')\nplt.plot(E, L)\nplt.plot(E, loss_classifier_mean, label='loss_classifier')\nplt.plot(E, loss_box_reg_mean, label='loss_box_reg')\nplt.plot(E, loss_objectness_mean, label='loss_objectness')\nplt.plot(E, loss_rpn_box_reg_mean, label='loss_rpn_box_reg')\nplt.legend(labels=[\"loss_train\",\"loss_classifier\",\"loss_box_reg\",\"loss_objectness\",\"loss_rpn_box_reg\"],loc=\"upper right\",fontsize=6)\nplt.show()\n\nplt.title('losses of test')\nplt.xlabel('epoch')\nplt.ylabel('losses')\nplt.plot(E, LT)\nplt.plot(E, loss_classifier_mean_test, label='loss_classifier')\nplt.plot(E, loss_box_reg_mean_test, label='loss_box_reg')\nplt.plot(E, loss_objectness_mean_test, label='loss_objectness')\nplt.plot(E, loss_rpn_box_reg_mean_test, label='loss_rpn_box_reg')\nplt.legend(labels=[\"loss_test\",\"loss_classifier\",\"loss_box_reg\",\"loss_objectness\",\"loss_rpn_box_reg\"],loc=\"upper right\",fontsize=6)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T07:19:20.906363Z","iopub.execute_input":"2022-06-10T07:19:20.906787Z","iopub.status.idle":"2022-06-10T07:19:22.552337Z","shell.execute_reply.started":"2022-06-10T07:19:20.906755Z","shell.execute_reply":"2022-06-10T07:19:22.551194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nms_prediction = apply_nms(prediction, iou_thresh=0.3)\nprint('NMS APPLIED MODEL OUTPUT')\nprint (\"Predicted NMS Labels: \",len(nms_prediction['labels']))\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T07:04:55.736364Z","iopub.execute_input":"2022-06-10T07:04:55.73705Z","iopub.status.idle":"2022-06-10T07:04:55.974947Z","shell.execute_reply.started":"2022-06-10T07:04:55.736971Z","shell.execute_reply":"2022-06-10T07:04:55.973653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to plot image","metadata":{}},{"cell_type":"code","source":"def plot_image(img_tensor, annotation,predict=True):\n    \n    fig,ax = plt.subplots(1)\n    fig.set_size_inches(18.5, 10.5)\n    img = img_tensor.cpu().data\n    mask_dic = {1:'without_mask', 2:'with_mask', 3:'mask_worn_incorrectly'}\n\n    # Display the image\n    ax.imshow(img.permute(1, 2, 0), cmap='gray')\n    \n    for i,box in enumerate(annotation[\"boxes\"]):\n        xmin, ymin, xmax, ymax = box\n\n        # Create a Rectangle patch\n        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=2,edgecolor='white',facecolor='none')\n\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n        label = mask_dic[int(annotation['labels'][i].data)]\n        if predict:\n            score = int((annotation['scores'][i].data) * 100)\n            if label == 'with_mask':\n                ax.text(xmin, ymin, f\"{label} : {score}%\", horizontalalignment='center', verticalalignment='center',fontsize=20,color='g')\n            else:\n                ax.text(xmin, ymin, f\"{label} : {score}%\", horizontalalignment='center', verticalalignment='center',fontsize=20,color='r')\n        else:\n            score=''\n            if label == 'with_mask':\n                ax.text(xmin, ymin, f\"{label}\", horizontalalignment='center', verticalalignment='center',fontsize=20,color='g')\n            else:\n                ax.text(xmin, ymin, f\"{label}\", horizontalalignment='center', verticalalignment='center',fontsize=20,color='r')\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-10T07:04:55.978115Z","iopub.execute_input":"2022-06-10T07:04:55.978557Z","iopub.status.idle":"2022-06-10T07:04:55.993134Z","shell.execute_reply.started":"2022-06-10T07:04:55.978516Z","shell.execute_reply":"2022-06-10T07:04:55.991664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 0;\nwhile idx < len(imgs):\n    nms_prediction = apply_nms(preds[idx], iou_thresh=0.3)\n    print(f'Prediction {idx+1}')\n    plot_image(imgs[idx], nms_prediction)\n    print(f'Target {idx+1}')\n    plot_image(imgs[idx].to('cpu'), annotations[idx],False)\n    \n    idx = idx + 1","metadata":{"execution":{"iopub.status.busy":"2022-06-10T07:04:56.963061Z","iopub.execute_input":"2022-06-10T07:04:56.963588Z","iopub.status.idle":"2022-06-10T07:05:00.87852Z","shell.execute_reply.started":"2022-06-10T07:04:56.963536Z","shell.execute_reply":"2022-06-10T07:05:00.877108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}